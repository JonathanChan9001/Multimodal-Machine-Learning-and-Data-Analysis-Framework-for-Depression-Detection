{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eaa8ae5",
   "metadata": {},
   "source": [
    "# Segment-Level Feature Extraction for Depression Detection\n",
    "\n",
    "**Author:** Jonathan Chan Jia Hao  \n",
    "**Affiliation:** Monash University Malaysia, School of Information Technology  \n",
    "**Date:** October 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements **segment-level feature extraction** for multimodal depression detection using the Extended DAIC-WOZ (E-DAIC) dataset. Unlike session-level approaches that aggregate entire conversations into single representations, this pipeline extracts fine-grained embeddings from individual utterances (text) and temporal segments (audio), preserving sequential and temporal dynamics for downstream attention-based modeling.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Depression manifests through subtle variations in linguistic patterns and acoustic characteristics that evolve throughout clinical interviews. Session-level aggregation (mean/max pooling) may lose critical temporal information. This notebook addresses this limitation by:\n",
    "\n",
    "- **Preserving Temporal Structure:** Extracting embeddings at utterance/segment granularity\n",
    "- **Enabling Attention Mechanisms:** Creating sequences suitable for learned attention pooling\n",
    "- **Improving Interpretability:** Allowing identification of depression-discriminative moments in conversations\n",
    "\n",
    "## Architecture Pipeline\n",
    "\n",
    "### Text Modality (Utterance-Level)\n",
    "1. **Input:** Participant transcripts (CSV format)\n",
    "2. **Preprocessing:** Parse and split into individual utterances\n",
    "3. **Embedding Model:** RoBERTa-base (768-dimensional)\n",
    "4. **Output:** Variable-length sequence of utterance embeddings `[N_utterances × 768]`\n",
    "\n",
    "### Audio Modality (Segment-Level)\n",
    "1. **Input:** Interview audio recordings (WAV format)\n",
    "2. **Preprocessing:** Split into 15-second fixed-length segments\n",
    "3. **Embedding Model:** Wav2Vec2-base-960h (768-dimensional)\n",
    "4. **Output:** Variable-length sequence of segment embeddings `[N_segments × 768]`\n",
    "\n",
    "## Key Features\n",
    "\n",
    "**Batch Processing Optimization**\n",
    "- GPU-accelerated batch inference for audio segments (configurable batch size)\n",
    "- Memory-efficient processing with automatic cache clearing\n",
    "- Nested progress tracking for large-scale feature extraction\n",
    "\n",
    "**Flexible Pooling Strategy**\n",
    "- Stores both raw segment embeddings (for attention learning) and mean-pooled baselines\n",
    "- Enables comparison between learned attention and simple averaging\n",
    "- Maintains compatibility with downstream classification pipelines\n",
    "\n",
    "**Robust Error Handling**\n",
    "- Graceful handling of missing transcripts/audio files\n",
    "- Automatic padding/truncation for variable-length inputs\n",
    "- Comprehensive logging and verification steps\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "**Input Files:**\n",
    "- Transcripts: `/daic_data/{PID}_P/{PID}_Transcript.csv`\n",
    "- Audio: `/daic_data/{PID}_P/{PID}_AUDIO.wav`\n",
    "- Labels: `/Labels/train_split.csv`, `dev_split.csv`, `test_split.csv`\n",
    "\n",
    "**Output Files:**\n",
    "- Text: `text_data_utterance_with_labels.pkl`\n",
    "- Audio: `audio_data_segment_with_labels.pkl`\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "### Part 1: Text Utterance Extraction (Cells 1-9)\n",
    "1. **Environment Setup** (Cell 1): Library imports, device configuration\n",
    "2. **Data Loading** (Cell 2): Load participant splits and labels\n",
    "3. **Model Initialization** (Cell 3-4): RoBERTa model and attention pooling layer\n",
    "4. **Preprocessing** (Cell 5): Transcript parsing and utterance splitting\n",
    "5. **Feature Extraction** (Cell 6-7): Utterance-level embedding generation\n",
    "6. **Save & Verify** (Cell 8-9): Export to pickle and validation\n",
    "\n",
    "### Part 2: Audio Segment Extraction (Cells 10-17)\n",
    "1. **Environment Setup** (Cell 10-11): Audio processing libraries and paths\n",
    "2. **Model Initialization** (Cell 12): Wav2Vec2 model loading\n",
    "3. **Audio Segmentation** (Cell 13): 15-second fixed-length chunking\n",
    "4. **Feature Extraction** (Cell 14-15): Batch-optimized segment embedding\n",
    "5. **Save & Verify** (Cell 16-17): Export to pickle and validation\n",
    "\n",
    "### Part 3: Memory Management (Cell 18)\n",
    "- Emergency cleanup procedures for interrupted processing\n",
    "- GPU cache clearing and memory monitoring\n",
    "\n",
    "## Technical Specifications\n",
    "\n",
    "**Text Processing:**\n",
    "- Tokenizer: RoBERTa byte-level BPE (50,265 vocab)\n",
    "- Max sequence length: 128 tokens\n",
    "- Embedding extraction: [CLS] token representation\n",
    "- Model parameters: 125M (frozen during feature extraction)\n",
    "\n",
    "**Audio Processing:**\n",
    "- Sampling rate: 16 kHz (downsampled if necessary)\n",
    "- Segment duration: 15 seconds (240,000 samples)\n",
    "- Feature extraction: Wav2Vec2 contextualized representations\n",
    "- Temporal pooling: Mean over time steps\n",
    "- Model parameters: 95M (frozen during feature extraction)\n",
    "\n",
    "## Output Schema\n",
    "\n",
    "Both pickle files contain DataFrames with the following structure:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'Participant_ID': str,              # Participant identifier\n",
    "    'utterance_embeddings' or 'segment_embeddings': np.array,  # [N × 768]\n",
    "    'utterances': List[str],            # (Text only) Raw utterance text\n",
    "    'num_utterances' or 'num_segments': int,  # Sequence length\n",
    "    'mean_pooled_embedding': np.array,  # [768] baseline aggregation\n",
    "    'Depression_label': int,            # Binary PHQ-8 label (0/1)\n",
    "    'PHQ8_Score': float,               # Continuous depression score\n",
    "    'split': str                        # 'train', 'dev', or 'test'\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**License:** MIT | **Contact:** jcha0091@student.monash.edu  \n",
    "**Repository:** [GitHub](https://github.com/JonathanChan9001/Multimodal-Machine-Learning-and-Data-Analysis-Framework-for-Depression-Detection)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c60abc",
   "metadata": {},
   "source": [
    "# 1. TEXT UTTERANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b76564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathanchan/miniconda/envs/comfy50/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d24f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total participants: 275\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define paths\n",
    "DATA_DIR = \"/home/jonathanchan/ml_data\"\n",
    "LABELS_DIR = os.path.join(DATA_DIR, \"Labels\")\n",
    "OUTPUT_FILE = os.path.join(DATA_DIR, \"text_data_utterance_with_labels.pkl\")\n",
    "\n",
    "# Load participant IDs and labels\n",
    "train_split = pd.read_csv(os.path.join(LABELS_DIR, \"train_split.csv\"))\n",
    "dev_split = pd.read_csv(os.path.join(LABELS_DIR, \"dev_split.csv\"))\n",
    "test_split = pd.read_csv(os.path.join(LABELS_DIR, \"test_split.csv\"))\n",
    "\n",
    "# Combine all splits\n",
    "all_splits = pd.concat([train_split, dev_split, test_split], ignore_index=True)\n",
    "print(f\"Total participants: {len(all_splits)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a86bbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathanchan/miniconda/envs/comfy50/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded roberta-base\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load RoBERTa model\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "roberta_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "roberta_model.eval()\n",
    "\n",
    "print(f\"Loaded {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32ffe2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention pooling layer initialized\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define Attention Pooling Layer\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"\n",
    "        embeddings: [num_utterances, hidden_dim]\n",
    "        returns: [hidden_dim]\n",
    "        \"\"\"\n",
    "        if len(embeddings.shape) == 1:\n",
    "            # Single utterance case\n",
    "            return embeddings\n",
    "        \n",
    "        attn_weights = torch.softmax(self.attention(embeddings), dim=0)\n",
    "        pooled = torch.sum(attn_weights * embeddings, dim=0)\n",
    "        return pooled\n",
    "\n",
    "attention_pooler = AttentionPooling(768).to(device)\n",
    "print(\"Attention pooling layer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43640628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Function to split transcript into utterances (FIXED for column structure)\n",
    "def split_into_utterances(transcript_path, participant_id):\n",
    "    \"\"\"\n",
    "    Parse transcript CSV and extract participant utterances.\n",
    "    Handles potential multi-level column headers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read transcript - skip potential extra header rows\n",
    "        df = pd.read_csv(transcript_path, sep='\\t', encoding='utf-8')\n",
    "        \n",
    "        # Debug: check actual column structure\n",
    "        actual_columns = df.columns.tolist()\n",
    "        \n",
    "        # Find the 'Text' column (might be nested or have spaces)\n",
    "        text_column = None\n",
    "        for col in actual_columns:\n",
    "            col_str = str(col).strip()\n",
    "            if 'Text' in col_str or 'text' in col_str.lower():\n",
    "                text_column = col\n",
    "                break\n",
    "        \n",
    "        if text_column is None:\n",
    "            # Fallback: assume third column is text\n",
    "            text_column = actual_columns[2] if len(actual_columns) > 2 else actual_columns[0]\n",
    "        \n",
    "        # Extract text utterances\n",
    "        utterances = df[text_column].tolist()\n",
    "        \n",
    "        # Clean utterances - remove NaN and empty strings\n",
    "        cleaned_utterances = []\n",
    "        for utt in utterances:\n",
    "            if pd.notna(utt) and len(str(utt).strip()) > 0:\n",
    "                cleaned_utterances.append(str(utt).strip())\n",
    "        \n",
    "        if len(cleaned_utterances) == 0:\n",
    "            print(f\"Warning: No utterances found for {participant_id}\")\n",
    "            return [\"\"]  # Return empty string to avoid crashes\n",
    "        \n",
    "        return cleaned_utterances\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {participant_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Show full error for debugging\n",
    "        return [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838b65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Function to get utterance embeddings\n",
    "def get_utterance_embeddings(utterances, max_length=128):\n",
    "    \"\"\"\n",
    "    Get RoBERTa embeddings for each utterance.\n",
    "    Returns: tensor of shape [num_utterances, 768]\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for utt in utterances:\n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                utt,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get RoBERTa output\n",
    "            outputs = roberta_model(**inputs)\n",
    "            \n",
    "            # Extract [CLS] token embedding\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze(0)  # [768]\n",
    "            embeddings.append(cls_embedding.cpu())\n",
    "    \n",
    "    # Stack into tensor\n",
    "    embeddings = torch.stack(embeddings)  # [num_utterances, 768]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b750145b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing participants: 100%|██████████| 275/275 [02:33<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 275 participants successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Process all participants\n",
    "# Find transcript files in daic_data directory\n",
    "TRANSCRIPT_DIR = os.path.join(DATA_DIR, \"daic_data\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in tqdm(all_splits.iterrows(), total=len(all_splits), desc=\"Processing participants\"):\n",
    "    participant_id = row['Participant_ID']\n",
    "    \n",
    "    # Construct transcript path - CORRECTED: Transcript.csv not TRANSCRIPT.csv\n",
    "    transcript_path = os.path.join(\n",
    "        TRANSCRIPT_DIR,\n",
    "        f\"{participant_id}_P\",\n",
    "        f\"{participant_id}_Transcript.csv\"\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(transcript_path):\n",
    "        print(f\"Transcript not found: {transcript_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Split into utterances\n",
    "    utterances = split_into_utterances(transcript_path, participant_id)\n",
    "    \n",
    "    # Get embeddings per utterance\n",
    "    utterance_embeddings = get_utterance_embeddings(utterances)\n",
    "    \n",
    "    # Store results with BOTH utterance-level and pooled embeddings\n",
    "    results.append({\n",
    "        'Participant_ID': participant_id,\n",
    "        'utterances': utterances,  # Original text\n",
    "        'utterance_embeddings': utterance_embeddings.numpy(),  # [num_utt, 768]\n",
    "        'num_utterances': len(utterances),\n",
    "        'Depression_label': row.get('PHQ8_Binary', None),\n",
    "        'PHQ8_Score': row.get('PHQ8_Score', None),\n",
    "        'split': 'train' if participant_id in train_split['Participant_ID'].values else \n",
    "                'dev' if participant_id in dev_split['Participant_ID'].values else 'test'\n",
    "    })\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} participants successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5c06d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame shape: (275, 8)\n",
      "\n",
      "Columns: ['Participant_ID', 'utterances', 'utterance_embeddings', 'num_utterances', 'Depression_label', 'PHQ8_Score', 'split', 'mean_pooled_embedding']\n",
      "\n",
      "Sample row:\n",
      "  Participant: 302\n",
      "  Num utterances: 99\n",
      "  Utterance embeddings shape: (99, 768)\n",
      "  Mean pooled shape: (768,)\n",
      "\n",
      "Saved to: /home/jonathanchan/ml_data/text_data_utterance_with_labels.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Create DataFrame and save\n",
    "df_utterance = pd.DataFrame(results)\n",
    "\n",
    "# Add mean pooling baseline for comparison\n",
    "df_utterance['mean_pooled_embedding'] = df_utterance['utterance_embeddings'].apply(\n",
    "    lambda x: np.mean(x, axis=0)\n",
    ")\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df_utterance.shape}\")\n",
    "print(f\"\\nColumns: {df_utterance.columns.tolist()}\")\n",
    "print(f\"\\nSample row:\")\n",
    "print(f\"  Participant: {df_utterance.iloc[0]['Participant_ID']}\")\n",
    "print(f\"  Num utterances: {df_utterance.iloc[0]['num_utterances']}\")\n",
    "print(f\"  Utterance embeddings shape: {df_utterance.iloc[0]['utterance_embeddings'].shape}\")\n",
    "print(f\"  Mean pooled shape: {df_utterance.iloc[0]['mean_pooled_embedding'].shape}\")\n",
    "\n",
    "# Save to pickle\n",
    "with open(OUTPUT_FILE, 'wb') as f:\n",
    "    pickle.dump(df_utterance, f)\n",
    "\n",
    "print(f\"\\nSaved to: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc84cbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Verification ===\n",
      "Loaded DataFrame shape: (275, 8)\n",
      "First participant ID: 302\n",
      "Split distribution:\n",
      "split\n",
      "train    163\n",
      "dev       56\n",
      "test      56\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Depression label distribution:\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Verify the file\n",
    "print(\"\\n=== Verification ===\")\n",
    "with open(OUTPUT_FILE, 'rb') as f:\n",
    "    loaded_df = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded DataFrame shape: {loaded_df.shape}\")\n",
    "print(f\"First participant ID: {loaded_df.iloc[0]['Participant_ID']}\")\n",
    "print(f\"Split distribution:\\n{loaded_df['split'].value_counts()}\")\n",
    "print(f\"\\nDepression label distribution:\\n{loaded_df['Depression_label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1d2aa",
   "metadata": {},
   "source": [
    "# 2. AUDIO ATTENTION POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03da03ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathanchan/miniconda/envs/comfy50/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio processing libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Import audio processing libraries\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Audio processing libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca9afaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total participants for audio processing: 275\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Define paths for audio processing\n",
    "DATA_DIR = \"/home/jonathanchan/ml_data\"\n",
    "AUDIO_DIR = os.path.join(DATA_DIR, \"daic_data\")\n",
    "LABELS_DIR = os.path.join(DATA_DIR, \"Labels\")\n",
    "OUTPUT_FILE = os.path.join(DATA_DIR, \"audio_data_segment_with_labels.pkl\")\n",
    "\n",
    "# Load splits\n",
    "train_split = pd.read_csv(os.path.join(LABELS_DIR, \"train_split.csv\"))\n",
    "dev_split = pd.read_csv(os.path.join(LABELS_DIR, \"dev_split.csv\"))\n",
    "test_split = pd.read_csv(os.path.join(LABELS_DIR, \"test_split.csv\"))\n",
    "all_splits = pd.concat([train_split, dev_split, test_split], ignore_index=True)\n",
    "\n",
    "print(f\"Total participants for audio processing: {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91622f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded facebook/wav2vec2-base-960h\n",
      "Model on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Load Wav2Vec2 model\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "wav2vec2_model = Wav2Vec2Model.from_pretrained(model_name).to(device)\n",
    "wav2vec2_model.eval()\n",
    "\n",
    "print(f\"Loaded {model_name}\")\n",
    "print(f\"Model on device: {next(wav2vec2_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5545da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Function to segment audio into chunks\n",
    "def segment_audio(audio_path, segment_length=15.0, sr=16000):\n",
    "    \"\"\"\n",
    "    Load audio and split into fixed-length segments.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: path to audio file\n",
    "        segment_length: length of each segment in seconds (default 15s)\n",
    "        sr: sampling rate (16000 for wav2vec2)\n",
    "    \n",
    "    Returns:\n",
    "        List of audio segments (numpy arrays)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        audio, original_sr = librosa.load(audio_path, sr=sr, mono=True)\n",
    "        \n",
    "        # Calculate segment size in samples\n",
    "        segment_samples = int(segment_length * sr)\n",
    "        \n",
    "        # Split into segments\n",
    "        segments = []\n",
    "        for i in range(0, len(audio), segment_samples):\n",
    "            segment = audio[i:i + segment_samples]\n",
    "            \n",
    "            # Pad last segment if too short (minimum 1 second)\n",
    "            if len(segment) < sr:  # Less than 1 second\n",
    "                continue\n",
    "            elif len(segment) < segment_samples:\n",
    "                # Pad with zeros\n",
    "                segment = np.pad(segment, (0, segment_samples - len(segment)))\n",
    "            \n",
    "            segments.append(segment)\n",
    "        \n",
    "        if len(segments) == 0:\n",
    "            print(f\"Warning: No valid segments extracted from {audio_path}\")\n",
    "            # Return one segment with silence\n",
    "            segments = [np.zeros(segment_samples)]\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error segmenting audio {audio_path}: {e}\")\n",
    "        # Return dummy segment\n",
    "        return [np.zeros(int(segment_length * sr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3928945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Function to extract wav2vec2 embeddings from audio segments (BATCH OPTIMIZED)\n",
    "\n",
    "def get_audio_segment_embeddings(audio_segments, max_samples=16000*15, batch_size=16):\n",
    "    \"\"\"\n",
    "    Extract wav2vec2 embeddings for each audio segment using batch processing.\n",
    "    \n",
    "    Args:\n",
    "        audio_segments: list of numpy arrays (audio waveforms)\n",
    "        max_samples: maximum length to process\n",
    "        batch_size: number of segments to process at once on GPU\n",
    "    \n",
    "    Returns:\n",
    "        tensor of shape [num_segments, 768]\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process segments in batches\n",
    "        for i in range(0, len(audio_segments), batch_size):\n",
    "            batch_segments = audio_segments[i:i + batch_size]\n",
    "            \n",
    "            # Prepare batch\n",
    "            processed_batch = []\n",
    "            for segment in batch_segments:\n",
    "                # Ensure segment is correct length\n",
    "                if len(segment) > max_samples:\n",
    "                    segment = segment[:max_samples]\n",
    "                processed_batch.append(segment)\n",
    "            \n",
    "            # Process entire batch at once (faster GPU utilization)\n",
    "            inputs = processor(\n",
    "                processed_batch,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get wav2vec2 output for entire batch\n",
    "            outputs = wav2vec2_model(**inputs)\n",
    "            \n",
    "            # Mean pool over time dimension\n",
    "            # outputs.last_hidden_state shape: [batch_size, time_steps, 768]\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)  # [batch_size, 768]\n",
    "            \n",
    "            # Move to CPU and store\n",
    "            embeddings.extend([emb.cpu() for emb in batch_embeddings])\n",
    "    \n",
    "    # Stack into tensor\n",
    "    embeddings = torch.stack(embeddings)  # [num_segments, 768]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29b089e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Participant batches: 100%|██████████| 69/69 [08:14<00:00,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 275 participants successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Process all participants with nested progress bars\n",
    "\n",
    "results = []\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Outer loop: participant batches\n",
    "for batch_start in tqdm(range(0, len(all_splits), BATCH_SIZE), desc=\"Participant batches\"):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, len(all_splits))\n",
    "    batch_rows = all_splits.iloc[batch_start:batch_end]\n",
    "    \n",
    "    # Inner loop: individual participants (with nested progress bar)\n",
    "    for idx, row in tqdm(batch_rows.iterrows(), total=len(batch_rows), \n",
    "                         desc=f\"Batch {batch_start//BATCH_SIZE + 1}\", \n",
    "                         leave=False):\n",
    "        participant_id = row['Participant_ID']\n",
    "        \n",
    "        audio_path = os.path.join(\n",
    "            AUDIO_DIR,\n",
    "            f\"{participant_id}_P\",\n",
    "            f\"{participant_id}_AUDIO.wav\"\n",
    "        )\n",
    "        \n",
    "        if not os.path.exists(audio_path):\n",
    "            continue\n",
    "        \n",
    "        # Segment audio into 15-second chunks\n",
    "        audio_segments = segment_audio(audio_path, segment_length=15.0)\n",
    "        \n",
    "        # Get wav2vec2 embeddings per segment (GPU batch processing in Cell 14)\n",
    "        segment_embeddings = get_audio_segment_embeddings(audio_segments)\n",
    "        \n",
    "        results.append({\n",
    "            'Participant_ID': participant_id,\n",
    "            'segment_embeddings': segment_embeddings.numpy(),\n",
    "            'num_segments': len(audio_segments),\n",
    "            'Depression_label': row.get('PHQ8_Binary', None),\n",
    "            'PHQ8_Score': row.get('PHQ8_Score', None),\n",
    "            'split': 'train' if participant_id in train_split['Participant_ID'].values else \n",
    "                    'dev' if participant_id in dev_split['Participant_ID'].values else 'test'\n",
    "        })\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} participants successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12a0c1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame shape: (275, 7)\n",
      "\n",
      "Columns: ['Participant_ID', 'segment_embeddings', 'num_segments', 'Depression_label', 'PHQ8_Score', 'split', 'mean_pooled_embedding']\n",
      "\n",
      "Sample statistics:\n",
      "  Participant: 302\n",
      "  Num segments: 51\n",
      "  Segment embeddings shape: (51, 768)\n",
      "  Mean pooled shape: (768,)\n",
      "\n",
      "Saved to: /home/jonathanchan/ml_data/audio_data_segment_with_labels.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Create DataFrame and save\n",
    "df_audio_segment = pd.DataFrame(results)\n",
    "\n",
    "# Add mean pooling baseline for comparison\n",
    "df_audio_segment['mean_pooled_embedding'] = df_audio_segment['segment_embeddings'].apply(\n",
    "    lambda x: np.mean(x, axis=0)\n",
    ")\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df_audio_segment.shape}\")\n",
    "print(f\"\\nColumns: {df_audio_segment.columns.tolist()}\")\n",
    "print(f\"\\nSample statistics:\")\n",
    "print(f\"  Participant: {df_audio_segment.iloc[0]['Participant_ID']}\")\n",
    "print(f\"  Num segments: {df_audio_segment.iloc[0]['num_segments']}\")\n",
    "print(f\"  Segment embeddings shape: {df_audio_segment.iloc[0]['segment_embeddings'].shape}\")\n",
    "print(f\"  Mean pooled shape: {df_audio_segment.iloc[0]['mean_pooled_embedding'].shape}\")\n",
    "\n",
    "# Optional: Remove raw audio segments to save disk space\n",
    "# Uncomment if you don't need the raw waveforms\n",
    "# df_audio_segment = df_audio_segment.drop(columns=['audio_segments'])\n",
    "\n",
    "# Save to pickle\n",
    "with open(OUTPUT_FILE, 'wb') as f:\n",
    "    pickle.dump(df_audio_segment, f)\n",
    "\n",
    "print(f\"\\nSaved to: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6a7e9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Audio Segmentation Verification ===\n",
      "Loaded DataFrame shape: (275, 7)\n",
      "First participant ID: 302\n",
      "\n",
      "Split distribution:\n",
      "split\n",
      "train    163\n",
      "dev       56\n",
      "test      56\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Depression label distribution:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "Segment count statistics:\n",
      "  Mean segments per participant: 65.0\n",
      "  Min segments: 28\n",
      "  Max segments: 132\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: Verify the saved file\n",
    "print(\"\\n=== Audio Segmentation Verification ===\")\n",
    "with open(OUTPUT_FILE, 'rb') as f:\n",
    "    loaded_df = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded DataFrame shape: {loaded_df.shape}\")\n",
    "print(f\"First participant ID: {loaded_df.iloc[0]['Participant_ID']}\")\n",
    "print(f\"\\nSplit distribution:\\n{loaded_df['split'].value_counts()}\")\n",
    "print(f\"\\nDepression label distribution:\\n{loaded_df['Depression_label'].value_counts()}\")\n",
    "print(f\"\\nSegment count statistics:\")\n",
    "print(f\"  Mean segments per participant: {loaded_df['num_segments'].mean():.1f}\")\n",
    "print(f\"  Min segments: {loaded_df['num_segments'].min()}\")\n",
    "print(f\"  Max segments: {loaded_df['num_segments'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cbb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Python garbage collected\n",
      "✓ GPU cache cleared\n",
      "\n",
      "GPU Memory Status:\n",
      "  Allocated: 0.85 GB\n",
      "  Reserved: 0.98 GB\n",
      "  Free: 14.47 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell: Emergency GPU/CPU memory cleanup after interruption\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# 1. Delete any large variables that might still exist\n",
    "try:\n",
    "    del results, audio_segments, segment_embeddings, batch_rows\n",
    "    print(\"✓ Deleted large variables\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2. Clear Python garbage\n",
    "gc.collect()\n",
    "print(\"✓ Python garbage collected\")\n",
    "\n",
    "# 3. Clear GPU cache (if using CUDA)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()  # Wait for all operations to finish\n",
    "    print(\"✓ GPU cache cleared\")\n",
    "    \n",
    "    # Show current GPU status\n",
    "    print(f\"\\nGPU Memory Status:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(f\"  Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"✓ No GPU detected\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comfy50",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
